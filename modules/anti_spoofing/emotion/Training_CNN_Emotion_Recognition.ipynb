{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Training_CNN_Emotion_Recognition.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#**TRAINING THE EMOTION RECOGNITION MODEL**"
   ],
   "metadata": {
    "id": "X7YJAgn_MeGb"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CleXUVKAnK9"
   },
   "source": [
    "## PREPARATION\n",
    "In the following, we are going to set some constants in order to use them later on. We also have mounting google drive in order to save the training logs into our g-drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxagsqpcBOK2"
   },
   "source": [
    "### MOUNTING GOOGLE DRIVE\n",
    "The first thing to do is to give google colab permission to access our drive so as to save the training checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#MOUNTING GOOGLE DRIVE\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive',  force_remount=True) \n",
    "\n",
    "# force_remount is an argument to force google drive to mount once again."
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5orT7qbeik21",
    "outputId": "1aeb282d-1bfb-40a6-e9e5-4cc7ec9db9b2"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aQ37jq2B0Bx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### DOWNLOAD The DATASET\n",
    "\n",
    "We have created a `dataset.zip` file from all the face mood images. Now we download this file from our drive in order to start the training phase."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "# Download dataset from google drive\n",
    "\n",
    "!gdown --id 1H8XilueKOkQ57-9QB7SvxGgqubLFyiBP\n",
    "\n",
    "# unzip the archive file\n",
    "!unzip Dataset.zip\n",
    "\n",
    "# we don't need the archive file anymore\n",
    "!rm dataset.zip"
   ],
   "metadata": {
    "id": "U40w51A4imkf"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMga49tNM_Q7"
   },
   "source": [
    "### IMPORT SOME NECESSARY LIBRARIES\n",
    "\n",
    "To implement our model, we have to import some needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4bYaU6Feax5m"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten,BatchNormalization\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop,Adam,SGD\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLlTQfb1E8NY"
   },
   "source": [
    "## LOAD AND GENERATION\n",
    "\n",
    "Now we have everything to start the training phase,\n",
    "Let's define some constants for our deep architecture. Each of the constants has its own application which is summarized in the table below:\n",
    "\n",
    "CONSTANT_NAME | APPLICATION\n",
    "-------------------|------------------\n",
    "BATCH_SIZE       | # samples that will be passed through to the network at one time\n",
    "IMAGE_ROWS       | width of the image \n",
    "IMAGE_COLS       | shape of the image (image_height, image_width)\n",
    "N_CLASSES        | the output of the model, the classification layer of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "num_classes = 7\n",
    "img_rows,img_cols = 48,48\n",
    "batch_size = 64\n",
    "\n",
    "# The directory paths of training and vallidation data\n",
    "train_data_dir = '/content/train'\n",
    "validation_data_dir = '/content/test'\n"
   ],
   "metadata": {
    "id": "FPH9FU_hayq9"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YH-5SgoRG8_r"
   },
   "source": [
    "### DATA GENERATOR\n",
    "We all encountered a situation where we try to load a dataset but there is not enough memory in our machine. \n",
    "\n",
    "As the field of machine learning progresses, this problem becomes more and more common. This is already one of the challenges in the field of vision where large datasets of images and video files are processed.\n",
    "\n",
    "Here, we will use `Keras` to build data generators for loading and processing our images\n",
    "\n",
    "The `ImageDataGenerator` class is very useful in image classification. There are several ways to use this generator, depending on the method we use, here we will focus on `flow_from_directory` which takes a path to the directory containing images sorted in sub directories and image augmentation parameters."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "\t\t\t\t\trescale=1./255,  # to transform every pixel value from range [0,255] -> [0,1]\n",
    "\t\t\t\t\trotation_range=30, # rotation\n",
    "\t\t\t\t\tshear_range=0.3,  # distorting the image \n",
    "\t\t\t\t\tzoom_range=0.3,  # zooming\n",
    "\t\t\t\t\twidth_shift_range=0.4,\n",
    "\t\t\t\t\theight_shift_range=0.4,\n",
    "\t\t\t\t\thorizontal_flip=True, # flips both rows and columns horizontally \n",
    "\t\t\t\t\tfill_mode='nearest')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n"
   ],
   "metadata": {
    "id": "HvBeeilkb5Ua"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "\t\t\t\t\ttrain_data_dir,\n",
    "\t\t\t\t\tcolor_mode='grayscale',\n",
    "\t\t\t\t\ttarget_size=(img_rows,img_cols),\n",
    "\t\t\t\t\tbatch_size=batch_size,\n",
    "\t\t\t\t\tclass_mode='categorical',\n",
    "\t\t\t\t\tshuffle=True)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "\t\t\t\t\t\t\tvalidation_data_dir,\n",
    "\t\t\t\t\t\t\tcolor_mode='grayscale',\n",
    "\t\t\t\t\t\t\ttarget_size=(img_rows,img_cols),\n",
    "\t\t\t\t\t\t\tbatch_size=batch_size,\n",
    "\t\t\t\t\t\t\tclass_mode='categorical',\n",
    "\t\t\t\t\t\t\tshuffle=True)"
   ],
   "metadata": {
    "id": "GluhE5OcknQu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1023cd3e-f874-436d-d2f3-8cf05c87c01b"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dzjXyjwKgYL"
   },
   "source": [
    "As you can see we have **28709** images which correspond to **7** different moods in our dataset where will be used as the training samples and **7178** images will be used as the validation samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZDWJhHdPIwp"
   },
   "source": [
    "## MODEL ARCHITECTURE AND TRAINING\n",
    "While deep learning is certainly not new, it is experiencing explosive growth because of the intersection of deeply layered neural networks and the use of GPUs to accelerate their execution.\n",
    "\n",
    "---\n",
    "We decide to create a deep architecure to be able to train a model with good accuracy. To do so we are going to take the most use out of some callback function in the keras api.\n",
    "- <b>`ModelCheckpoint`</b> üèÅ: callback is used in conjunction with training using `model.fit()` to save a model or weights (in a checkpoint file) at some interval, so the model or weights can be loaded later to continue the training from the state saved.\n",
    "\n",
    "- <b>`EarlyStopping` </b>üö¶: Assuming the goal of a training is to minimize the loss. With this, the metric to be monitored would be `loss`, and mode would be `min`. A `model.fit()` training loop will check at end of every epoch whether the `loss` is no longer decreasing, considering the `min_delta` and patience if applicable. Once it's found no longer decreasing,the training terminates.\n",
    "\n",
    "- <b> `ReduceLROnPlateau` </b> íëà: Models often benefit from reducing the learning rate by a factor of 2-10 once learning decreases. This callback monitors a quantity and if no improvement is seen for a `patience` number of epochs, the learning rate is reduced.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# reduces learning rate if no improvement are seen\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "\n",
    "# stop training if no improvements are seen\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=3,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )\n",
    "\n",
    "# saves model weights to file\n",
    "checkpoint = ModelCheckpoint(os.path.join('/content/gdrive/MyDrive/Multimodal_Interaction/model', 'cp-{epoch:04d}.h5'),\n",
    "                            monitor='val_loss',\n",
    "                            verbose=1,\n",
    "                            save_best_only=True,\n",
    "                            mode='min',\n",
    "                            )\n",
    "\n",
    "callbacks = [earlystop,checkpoint,learning_rate_reduction]"
   ],
   "metadata": {
    "id": "ZgBktqFxmKak"
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECVcSaJZVVsE"
   },
   "source": [
    "### MODEL ARCHITECTURE\n",
    "\n",
    "*   contains 4 convolutional layers\n",
    "*   2 dense layers\n",
    "*   and an output (classification) layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "\n",
    "################################.    1st CONVOLUTIONAL LAYER     ################################\n",
    "\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "################################.    2nd CONVOLUTIONAL LAYER     ################################\n",
    "\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "################################.    3rd CONVOLUTIONAL LAYER     ################################\n",
    "\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "################################.    4th CONVOLUTIONAL LAYER     ################################\n",
    "\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "################################    FLATTEN, FOR DENSE LAYER     ################################\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "################################.    DENSE LAYERS: 1st LAYER     ################################\n",
    "\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "################################.    DENSE LAYERS: 2nd LAYER     ################################\n",
    "\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "################################.    OUTPUT LAYER: CLASSES    ################################\n",
    "\n",
    "model.add(Dense(num_classes,kernel_initializer='he_normal'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())\n"
   ],
   "metadata": {
    "id": "KJchlIGtcDak",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "26209b74-1400-42e0-acd2-5c1a92a05c58"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 48, 48, 32)        320       \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 48, 48, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 48, 48, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 48, 48, 32)        9248      \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 48, 48, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 48, 48, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 24, 24, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 24, 24, 32)        0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 24, 24, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 24, 24, 64)        36928     \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 24, 24, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 12, 12, 64)        0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 12, 12, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 12, 12, 128)       147584    \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 12, 12, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 6, 6, 128)         0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 6, 6, 256)         295168    \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 6, 6, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 6, 6, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 3, 3, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 3, 3, 256)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                147520    \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      " activation_21 (Activation)  (None, 7)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,328,167\n",
      "Trainable params: 1,325,991\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-czLynttWJp5"
   },
   "source": [
    "### TRAINING\n",
    "\n",
    "We are ready to start the training phase. At the first, we compile the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.001),\n",
    "              metrics=['accuracy'])"
   ],
   "metadata": {
    "id": "BqcE6ay0cS4J",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cfd479d6-458c-46f4-dfeb-cebb43dc6b86"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "nb_train_samples = 28709\n",
    "nb_validation_samples = 7178\n",
    "epochs=150"
   ],
   "metadata": {
    "id": "dLhFnQlWnK6_"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "history=model.fit_generator(\n",
    "                train_generator,\n",
    "                steps_per_epoch=nb_train_samples//batch_size,\n",
    "                epochs=epochs,\n",
    "                callbacks=callbacks,\n",
    "                validation_data=validation_generator,\n",
    "                validation_steps=nb_validation_samples//batch_size)"
   ],
   "metadata": {
    "id": "rsI5cy0qnhv8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c65d294b-663c-40e8-8320-cf9efbcb2508"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  import sys\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "448/448 [==============================] - ETA: 0s - loss: 2.2488 - accuracy: 0.1846\n",
      "Epoch 1: val_loss improved from inf to 1.79494, saving model to /content/gdrive/MyDrive/Multimodal_Interaction/model/cp-0001.h5\n",
      "448/448 [==============================] - 28s 57ms/step - loss: 2.2488 - accuracy: 0.1846 - val_loss: 1.7949 - val_accuracy: 0.2443 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "447/448 [============================>.] - ETA: 0s - loss: 1.8664 - accuracy: 0.2304\n",
      "Epoch 2: val_loss improved from 1.79494 to 1.77302, saving model to /content/gdrive/MyDrive/Multimodal_Interaction/model/cp-0002.h5\n",
      "448/448 [==============================] - 25s 57ms/step - loss: 1.8664 - accuracy: 0.2304 - val_loss: 1.7730 - val_accuracy: 0.2543 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "447/448 [============================>.] - ETA: 0s - loss: 1.8054 - accuracy: 0.2506\n",
      "Epoch 3: val_loss improved from 1.77302 to 1.76187, saving model to /content/gdrive/MyDrive/Multimodal_Interaction/model/cp-0003.h5\n",
      "448/448 [==============================] - 25s 57ms/step - loss: 1.8053 - accuracy: 0.2506 - val_loss: 1.7619 - val_accuracy: 0.2624 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "447/448 [============================>.] - ETA: 0s - loss: 1.7829 - accuracy: 0.2574\n",
      "Epoch 4: val_loss improved from 1.76187 to 1.71743, saving model to /content/gdrive/MyDrive/Multimodal_Interaction/model/cp-0004.h5\n",
      "448/448 [==============================] - 25s 56ms/step - loss: 1.7832 - accuracy: 0.2574 - val_loss: 1.7174 - val_accuracy: 0.2949 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.7495 - accuracy: 0.2789\n",
      "Epoch 5: val_loss did not improve from 1.71743\n",
      "448/448 [==============================] - 25s 56ms/step - loss: 1.7495 - accuracy: 0.2789 - val_loss: 1.9150 - val_accuracy: 0.2578 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "447/448 [============================>.] - ETA: 0s - loss: 1.7193 - accuracy: 0.2965\n",
      "Epoch 6: val_loss improved from 1.71743 to 1.70346, saving model to /content/gdrive/MyDrive/Multimodal_Interaction/model/cp-0006.h5\n",
      "448/448 [==============================] - 25s 56ms/step - loss: 1.7191 - accuracy: 0.2965 - val_loss: 1.7035 - val_accuracy: 0.3224 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "447/448 [============================>.] - ETA: 0s - loss: 1.6780 - accuracy: 0.3243\n",
      "Epoch 7: val_loss improved from 1.70346 to 1.61246, saving model to /content/gdrive/MyDrive/Multimodal_Interaction/model/cp-0007.h5\n",
      "448/448 [==============================] - 25s 56ms/step - loss: 1.6779 - accuracy: 0.3244 - val_loss: 1.6125 - val_accuracy: 0.3726 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.5996 - accuracy: 0.3711\n",
      "Epoch 8: val_loss improved from 1.61246 to 1.40049, saving model to /content/gdrive/MyDrive/Multimodal_Interaction/model/cp-0008.h5\n",
      "448/448 [==============================] - 25s 56ms/step - loss: 1.5996 - accuracy: 0.3711 - val_loss: 1.4005 - val_accuracy: 0.4743 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.5155 - accuracy: 0.4098\n",
      "Epoch 9: val_loss improved from 1.40049 to 1.29590, saving model to /content/gdrive/MyDrive/Multimodal_Interaction/model/cp-0009.h5\n",
      "448/448 [==============================] - 25s 55ms/step - loss: 1.5155 - accuracy: 0.4098 - val_loss: 1.2959 - val_accuracy: 0.5018 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.4765 - accuracy: 0.4271\n",
      "Epoch 10: val_loss did not improve from 1.29590\n",
      "448/448 [==============================] - 25s 55ms/step - loss: 1.4765 - accuracy: 0.4271 - val_loss: 1.3430 - val_accuracy: 0.4799 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.4405 - accuracy: 0.4436\n",
      "Epoch 11: val_loss did not improve from 1.29590\n",
      "448/448 [==============================] - 24s 53ms/step - loss: 1.4405 - accuracy: 0.4436 - val_loss: 1.3371 - val_accuracy: 0.4904 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.4118 - accuracy: 0.4614\n",
      "Epoch 12: val_loss improved from 1.29590 to 1.23360, saving model to /content/gdrive/MyDrive/Multimodal_Interaction/model/cp-0012.h5\n",
      "448/448 [==============================] - 24s 54ms/step - loss: 1.4118 - accuracy: 0.4614 - val_loss: 1.2336 - val_accuracy: 0.5324 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "447/448 [============================>.] - ETA: 0s - loss: 1.3888 - accuracy: 0.4695\n",
      "Epoch 13: val_loss improved from 1.23360 to 1.20329, saving model to /content/gdrive/MyDrive/Multimodal_Interaction/model/cp-0013.h5\n",
      "448/448 [==============================] - 24s 53ms/step - loss: 1.3887 - accuracy: 0.4695 - val_loss: 1.2033 - val_accuracy: 0.5326 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.3689 - accuracy: 0.4777\n",
      "Epoch 14: val_loss did not improve from 1.20329\n",
      "448/448 [==============================] - 24s 53ms/step - loss: 1.3689 - accuracy: 0.4777 - val_loss: 1.2217 - val_accuracy: 0.5308 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "447/448 [============================>.] - ETA: 0s - loss: 1.3542 - accuracy: 0.4865\n",
      "Epoch 15: val_loss did not improve from 1.20329\n",
      "448/448 [==============================] - 24s 52ms/step - loss: 1.3541 - accuracy: 0.4866 - val_loss: 1.2097 - val_accuracy: 0.5352 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "447/448 [============================>.] - ETA: 0s - loss: 1.3441 - accuracy: 0.4912\n",
      "Epoch 16: val_loss improved from 1.20329 to 1.12379, saving model to /content/gdrive/MyDrive/Multimodal_Interaction/model/cp-0016.h5\n",
      "448/448 [==============================] - 24s 54ms/step - loss: 1.3436 - accuracy: 0.4913 - val_loss: 1.1238 - val_accuracy: 0.5703 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.3328 - accuracy: 0.4978\n",
      "Epoch 17: val_loss improved from 1.12379 to 1.10826, saving model to /content/gdrive/MyDrive/Multimodal_Interaction/model/cp-0017.h5\n",
      "448/448 [==============================] - 24s 54ms/step - loss: 1.3328 - accuracy: 0.4978 - val_loss: 1.1083 - val_accuracy: 0.5734 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "447/448 [============================>.] - ETA: 0s - loss: 1.3205 - accuracy: 0.4979\n",
      "Epoch 18: val_loss did not improve from 1.10826\n",
      "448/448 [==============================] - 25s 55ms/step - loss: 1.3200 - accuracy: 0.4981 - val_loss: 1.1110 - val_accuracy: 0.5707 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "447/448 [============================>.] - ETA: 0s - loss: 1.3140 - accuracy: 0.5027\n",
      "Epoch 19: val_loss did not improve from 1.10826\n",
      "448/448 [==============================] - 25s 55ms/step - loss: 1.3137 - accuracy: 0.5029 - val_loss: 1.1672 - val_accuracy: 0.5607 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.2976 - accuracy: 0.5162Restoring model weights from the end of the best epoch: 17.\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.10826\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "448/448 [==============================] - 25s 55ms/step - loss: 1.2976 - accuracy: 0.5162 - val_loss: 1.1336 - val_accuracy: 0.5668 - lr: 0.0010\n",
      "Epoch 20: early stopping\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# OUTPUT\n",
    "\n",
    "Our output is a model with the name of \"cp-0017.h5\" which will be used as our model for **Live Emotion Detection** in our Fatcha Application."
   ],
   "metadata": {
    "id": "tmBCyxRAZvgK"
   }
  }
 ]
}